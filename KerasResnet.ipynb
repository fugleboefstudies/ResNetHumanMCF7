{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_model():\n",
    "#     input_tensor = tf.keras.Input(shape=(68,68,3))\n",
    "#     #input_tensor = tf.keras.Input(shape=(224,224,3))\n",
    "#     #to_res = (224, 224)\n",
    "#     model_res = tf.keras.applications.ResNet50V2(include_top = False, weights = None, input_tensor = input_tensor)\n",
    "    \n",
    "#     # for layer in model_res.layers[:143]:\n",
    "#     #     layer.trainable = False\n",
    "    \n",
    "#     model = tf.keras.models.Sequential()\n",
    "#     #model.add(tf.keras.layers.Lambda(lambda image: tf.image.resize(image, to_res)))\n",
    "#     model.add(model_res)\n",
    "#     model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    \n",
    "#     # for i in range(100):\n",
    "#     #     model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "#     #     model.add(tf.keras.layers.Dropout(0.2))\n",
    "#     #     model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#     model.add(tf.keras.layers.Dense(12, activation = 'softmax'))\n",
    "    \n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_model(n_dense_layers):\n",
    "#     input_tensor = tf.keras.Input(shape=(68,68,3))\n",
    "#     model_res = tf.keras.applications.ResNet50V2(include_top = False, weights = None, input_tensor = input_tensor)\n",
    "#     model = tf.keras.models.Sequential()\n",
    "\n",
    "#     model.add(model_res) #Resnet\n",
    "#     model.add(tf.keras.layers.Flatten()) \n",
    "    \n",
    "    \n",
    "#     for i in range(n_dense_layers):\n",
    "#         model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "#         model.add(tf.keras.layers.Dropout(0.2))\n",
    "#         model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#     model.add(tf.keras.layers.Dense(12, activation = 'softmax'))\n",
    "    \n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(tf.keras.utils.Sequence):\n",
    "    #def __init__(self, df, model = None, one_hot_encoder, learning_rate_schedule = None, batch_size, input_size = (68,68,3), shuffle = True):\n",
    "    def __init__(self, df, one_hot_encoder, batch_size, model = None, input_size = (68,68,3), shuffle = True):\n",
    "        self.df = df\n",
    "        self.dmsomean = np.load('F:\\Programming\\DTU\\Human MCF7\\Segmented\\Inspection\\ClassMean\\DMSO.npy')\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.df)\n",
    "        self.n_classes = self.df['moa'].nunique()\n",
    "        \n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        \n",
    "        #self.learning_rate_schedule = iter(learning_rate_schedule)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        # lr = self.get_next_learning_rate()\n",
    "        # if lr:\n",
    "        #     tf.keras.backend.set_value(model.optimizer.learning_rate, lr)\n",
    "        #     print(f\"Learning rate adjusted to: {lr}\")\n",
    "            \n",
    "    # def get_next_learning_rate(self):\n",
    "    #     try:\n",
    "    #         lr = next(self.learning_rate_schedule)\n",
    "    #     except StopIteration:\n",
    "    #         lr = None\n",
    "    #     return lr\n",
    "\n",
    "    def __get_img(self, path):\n",
    "        image_arr = np.load(path)\n",
    "        image_arr = image_arr/255\n",
    "        image_arr -= self.dmsomean #Normalize by dmso\n",
    "\n",
    "        image_arr = tf.image.resize(image_arr, (self.input_size[0], self.input_size[1]))\n",
    "        return image_arr\n",
    "    \n",
    "    def __get_label(self, moa):\n",
    "        #print(moa)\n",
    "        label = self.one_hot_encoder.transform(moa.to_numpy().reshape(-1, 1))\n",
    "        return label\n",
    "    \n",
    "    def __get_batch(self, batch):\n",
    "\n",
    "        img_batch = batch['path'].apply(self.__get_img)\n",
    "        img_batch = np.array([img for img in img_batch])\n",
    "        img_batch = tf.keras.applications.resnet50.preprocess_input(img_batch)\n",
    "        \n",
    "\n",
    "        #label_batch = batch['moa'].apply(self.__get_label)\n",
    "        label_batch = self.one_hot_encoder.transform(batch.moa.to_numpy().reshape(-1, 1))\n",
    "        \n",
    "        return img_batch, label_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch = self.df[index * self.batch_size:(index+1)*self.batch_size]\n",
    "        X, Y = self.__get_batch(batch)\n",
    "        return X, Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data_paths.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df[df.moa != 'DMSO']\n",
    "#df = df.drop(df[df['moa'] == 'Microtubule stabilizers'].sample(frac=0.75).index)\n",
    "#df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Microtubule stabilizers      80089\n",
       "Microtubule destabilizers    15055\n",
       "Epithelial                   14929\n",
       "Aurora kinase inhibitors     12821\n",
       "Eg5 inhibitors               12545\n",
       "Kinase inhibitors            11622\n",
       "DNA damage                    9391\n",
       "Actin disruptors              7412\n",
       "Protein degradation           6611\n",
       "DNA replication               6019\n",
       "Cholesterol-lowering          5415\n",
       "Protein synthesis             3764\n",
       "Name: moa, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.moa.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# indices = np.arange(len(df))\n",
    "# X_train, X_test, y_train, y_test, idx_train, idx_test = sklearn.model_selection.train_test_split(df['path'], df['moa'], indices, test_size = 0.5, random_state=0, shuffle = True, stratify=df['moa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moa_one_hot_encoder = sklearn.preprocessing.OneHotEncoder(sparse=False)\n",
    "moa_one_hot_encoder.fit(df['moa'].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microtubule stabilizers      40044\n",
      "Microtubule destabilizers     7527\n",
      "Epithelial                    7464\n",
      "Aurora kinase inhibitors      6410\n",
      "Eg5 inhibitors                6272\n",
      "Kinase inhibitors             5811\n",
      "DNA damage                    4696\n",
      "Actin disruptors              3706\n",
      "Protein degradation           3306\n",
      "DNA replication               3010\n",
      "Cholesterol-lowering          2708\n",
      "Protein synthesis             1882\n",
      "Name: moa, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(df))\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = sklearn.model_selection.train_test_split(df['path'], df['moa'], indices, test_size = 0.5, random_state=0, shuffle = True, stratify=df['moa'])\n",
    "\n",
    "df_train = df.loc[idx_train]\n",
    "df_test = df.loc[idx_test]\n",
    "print(df_train.moa.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             moa\n",
      "Microtubule stabilizers    40045\n",
      "Microtubule destabilizers   7528\n",
      "Epithelial                  7465\n",
      "Aurora kinase inhibitors    6411\n",
      "Eg5 inhibitors              6273\n",
      "Kinase inhibitors           5811\n",
      "DNA damage                  4695\n",
      "Actin disruptors            3706\n",
      "Protein degradation         3305\n",
      "DNA replication             3009\n",
      "Cholesterol-lowering        2707\n",
      "Protein synthesis           1882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{9: 1.0,\n",
       " 8: 2.0,\n",
       " 6: 2.0,\n",
       " 1: 2.0,\n",
       " 5: 2.0,\n",
       " 7: 2.0,\n",
       " 3: 2.0,\n",
       " 0: 3.0,\n",
       " 10: 3.0,\n",
       " 4: 3.0,\n",
       " 2: 3.0,\n",
       " 11: 4.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_distribution = pd.DataFrame(y_test.value_counts())\n",
    "# max_count = df_distribution.max().moa\n",
    "# moa_mean = np.mean(df_distribution.moa)\n",
    "# moa_class_weights = {}\n",
    "# for moa, i in df_distribution.iterrows():\n",
    "#     label = moa_one_hot_encoder.transform(np.array([moa]).reshape(-1,1))\n",
    "#     moa_class_weights[np.argmax(label)] = np.floor(np.sqrt(max_count//i.moa))\n",
    "# print(df_distribution)\n",
    "# moa_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022.06.13_00.58.58\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = init_model()\n",
    "\n",
    "\n",
    "# #train_dataloader = DataLoader(df = df_train, model = model, one_hot_encoder = moa_one_hot_encoder, learning_rate_schedule, batch_size=32, input_size=(68,68,3))\n",
    "# train_dataloader = DataLoader(df = df_train, model = model, one_hot_encoder = moa_one_hot_encoder, batch_size=32, input_size=(68,68,3))\n",
    "# test_dataloader = DataLoader(df = df_test, one_hot_encoder = moa_one_hot_encoder, batch_size = 32, input_size=(68,68,3))\n",
    "\n",
    "\n",
    "# rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor = 0.1, patience = 5, min_lr=1e-6)\n",
    "# check_point = tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints/test.h5' , monitor = \"val_acc\", mode = \"max\", save_best_only=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-2, momentum=0.9, decay = 0.01),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# #history=model.fit(train_dataloader, validation_data=test_dataloader, epochs= 1, callbacks = [check_point], class_weight = moa_class_weights)\n",
    "# #history=model.fit(train_dataloader, validation_data=test_dataloader, epochs= 20, callbacks = [check_point])\n",
    "# history=model.fit(train_dataloader, validation_data=test_dataloader, epochs= 20, callbacks = [check_point, rlrop], class_weight = moa_class_weights)\n",
    "# model.summary()\n",
    "# model.save('models/test2.h5')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_results\\\\2022.06.13_01.15.11_checkpoints.h5'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 5.1778 - accuracy: 0.4266WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 499s 168ms/step - loss: 5.1778 - accuracy: 0.4266 - val_loss: 606.3686 - val_accuracy: 0.4314 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 3.7014 - accuracy: 0.4527WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 487s 168ms/step - loss: 3.7014 - accuracy: 0.4527 - val_loss: 247.7257 - val_accuracy: 0.4313 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 3.6106 - accuracy: 0.4637WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 488s 168ms/step - loss: 3.6106 - accuracy: 0.4637 - val_loss: 1346.2216 - val_accuracy: 0.2120 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 3.3765 - accuracy: 0.4926WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 484s 167ms/step - loss: 3.3765 - accuracy: 0.4926 - val_loss: 8.0627 - val_accuracy: 0.4314 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 3.1841 - accuracy: 0.5053WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 484s 167ms/step - loss: 3.1841 - accuracy: 0.5053 - val_loss: 549.1787 - val_accuracy: 0.4159 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 3.0501 - accuracy: 0.5216WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 487s 168ms/step - loss: 3.0501 - accuracy: 0.5216 - val_loss: 664.4312 - val_accuracy: 0.2499 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.9237 - accuracy: 0.5388WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 472s 163ms/step - loss: 2.9237 - accuracy: 0.5388 - val_loss: 1288.0299 - val_accuracy: 0.2134 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.7771 - accuracy: 0.5557WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 470s 162ms/step - loss: 2.7771 - accuracy: 0.5557 - val_loss: 95.2808 - val_accuracy: 0.4177 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.6118 - accuracy: 0.5800WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 504s 174ms/step - loss: 2.6118 - accuracy: 0.5800 - val_loss: 2.8402 - val_accuracy: 0.4283 - lr: 1.0000e-03\n",
      "Epoch 10/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.5815 - accuracy: 0.5844WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 479s 165ms/step - loss: 2.5815 - accuracy: 0.5844 - val_loss: 1.5404 - val_accuracy: 0.5595 - lr: 1.0000e-03\n",
      "Epoch 11/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.5602 - accuracy: 0.5873WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 479s 165ms/step - loss: 2.5602 - accuracy: 0.5873 - val_loss: 2.1175 - val_accuracy: 0.3043 - lr: 1.0000e-03\n",
      "Epoch 12/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.5441 - accuracy: 0.5910WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 477s 165ms/step - loss: 2.5441 - accuracy: 0.5910 - val_loss: 1.9407 - val_accuracy: 0.3424 - lr: 1.0000e-03\n",
      "Epoch 13/20\n",
      "2901/2901 [==============================] - ETA: 0s - loss: 2.5345 - accuracy: 0.5905WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2901/2901 [==============================] - 476s 164ms/step - loss: 2.5345 - accuracy: 0.5905 - val_loss: 2.2129 - val_accuracy: 0.2491 - lr: 1.0000e-03\n",
      "Epoch 14/20\n",
      " 646/2901 [=====>........................] - ETA: 6:16 - loss: 2.5040 - accuracy: 0.5938"
     ]
    }
   ],
   "source": [
    "import init_models\n",
    "for i in range(4):\n",
    "    model_id = datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
    "    base_path = 'train_results'\n",
    "    path_checkpoint = os.path.join(base_path, model_id+'_checkpoint.h5')\n",
    "    path_model = os.path.join(base_path, model_id+'_model.h5')  \n",
    "    path_log = os.path.join(base_path, model_id+'_log.csv')  \n",
    "    \n",
    "    model = init_models.init_model(i)\n",
    "\n",
    "\n",
    "    #train_dataloader = DataLoader(df = df_train, model = model, one_hot_encoder = moa_one_hot_encoder, learning_rate_schedule, batch_size=32, input_size=(68,68,3))\n",
    "    train_dataloader = DataLoader(df = df_train, model = model, one_hot_encoder = moa_one_hot_encoder, batch_size=32, input_size=(68,68,3))\n",
    "    test_dataloader = DataLoader(df = df_test, one_hot_encoder = moa_one_hot_encoder, batch_size = 32, input_size=(68,68,3))\n",
    "\n",
    "\n",
    "    rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor = 0.1, patience = 5, min_lr=1e-6)\n",
    "    check_point = tf.keras.callbacks.ModelCheckpoint(filepath=path_checkpoint , monitor = \"val_acc\", mode = \"max\", save_best_only=True)\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(path_log, append = True, separator=',')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-2, momentum=0.9, decay = 0.01),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    #history=model.fit(train_dataloader, validation_data=test_dataloader, epochs= 1, callbacks = [check_point], class_weight = moa_class_weights)\n",
    "    #history=model.fit(train_dataloader, validation_data=test_dataloader, epochs= 20, callbacks = [check_point])\n",
    "    history=model.fit(train_dataloader, validation_data=test_dataloader, epochs= 20, callbacks = [check_point, rlrop, csv_logger], class_weight = moa_class_weights)\n",
    "    model.summary()\n",
    "    model.save(path_model)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a630c9ca01470c5a1ece2edbc02ef0e6f1d3772bccb2b39f1446bf635119703"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mlgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
